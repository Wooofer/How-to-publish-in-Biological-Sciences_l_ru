# Are researchers writing more, and is more better? {#publishmore}

The idiom ‘publish or perish’ suggests that researchers will increase their output in order to obtain positions and promotions. And if a researcher’s productivity is measured by their publication output, shouldn't we all be writing more papers? Certainly, it appears that more papers are being published (see Figure \@ref(fig:authors-writing-more). An estimate for the total number of scholarly articles in existence was 50 million in 2009, with more than 25 million published in the years from 1984 [@jinha2010article].

Similarly, if we are all be writing more, then wouldn't some people start publishing two (or more) papers, when one would be adequate? This idea of ‘salami slicing’ to inflate outputs would be an understandable strategy if researchers were all trying to increase their output. Alternatively, the names of authors might be added to papers in which they did not make significant input via ghost [authorship or hyperauthorship](authors.html) [see @cronin2001hyperauthorship for an interesting historical perspective, and [Part I](authors.html)]. 


(ref:authors-writing-more) **The growth in the number of papers published in Life Sciences over time.** The number of papers published (blue line) compared to a standard growth rate (black line). The data come from www.scopus.com.

```{r authors-writing-more, echo=FALSE, fig.cap="(ref:authors-writing-more)"}
if(knitr::is_html_output(excludes="markdown")) knitr::include_url("https://johnmeasey.github.io/authors-writing-more/index.html", height="450px") else knitr::include_graphics("figures/authors-writing-more/authors-writing-more.png")
```


A study by Fanelli and Larivière [-@fanelli2016researchers] has a new take on the above questions, by asking whether researchers are actually writing more papers now than they did 100 years ago. They used Web of Science to look for unique authors (more than half a million of them) and determine whether the first year of publication and the total number of publications resulted in an increasing trend. But it is possible that these figures could be explained by the fact that the culture in publishing in biological sciences has changed a great deal since then. One hundred years ago, it was very unlikely that any postgraduate students would publish articles in peer reviewed journals. Moreover, it was also acceptable for advisers to take the thesis work of their students and write it up in monologues. This has certainly changed with the ranks of authors now being swelled considerably so that many more authors are likely to be included on only a single publication in which they participated. Hence, Fanelli and Larivière’s [-@fanelli2016researchers] trend line for biology is very stable at around 5.5 publications whether you started publishing in 1900 or 2000 (note that earth science and chemistry do both increase dramatically), but this may reflect an influx of junior authors into the publishing system. 

## Are some authors unfeasibly prolific?

Whether or not it is feasible for individuals to write so many articles, was the question posed in a study that examined prolific authors in four fields of medicine [@wager2015too]. This publication piqued my interest as it turns out that they decided that researchers with more than 25 publications in a year were “unfeasibly prolific” as this would be the equivalent of “>1 publication per 10 working days”. Their angle was to suggest that publication fraud was likely, and that funders should be more circumspect when accepting researchers productivity as a metric. Looking back through the peer review of this article (which is a great aspect of many PeerJ articles), I’m astounded that only one reviewer questioned the premise that it’s infeasible to author that number of papers in a year.

I have published >25 papers in a year, and I know other people who do this regularly. To me, there is no question that (a) it is possible and (b) that they really are the authors. Firstly, the idea that prolific authors constrain their activity to “working days” is naïve. Most will be working throughout a normal weekend, and working in the early morning and late evening, especially in China [see @barnett2019working]. A hallmark of a prolific author would be emails early in the morning and/or late at night. This gives you an indication of their working hours, and how they are struggling to keep up with correspondence on top of writing papers. 

Authorship of a publication is often the result of several years of work. Thus, publications that I co-authored in 2017 frequently had research conducted in 2014 or earlier. For example, one of the publications, Measey et al [-@measey2017counting] is the product of aSCR work that started in 2009, funded in 2011 with fieldwork in 2012, and required the development of software for analysis by Ben Stevenson and colleagues [-@stevenson2015general], before it could be completed and submitted. Thus, from my perspective, when I look at authoring a lot of publications it reflects the activity of the initial concept for the work, raising of money, conducting the field work or experiment, analysing the data and then writing it up (with the subsequent submission and peer review time). Thus, publications in 2017 result from a lot of work done for more than 3 years. 


## Salami-slicing {#salami}
During the production of any dataset, you are likely to find that you are able to answer more questions that you originally set out to ask when you first proposed the research (i.e. preregistration; see [Part I](preprints2.html)). The question you will be left with is: whether you should be adding these post hoc questions to the manuscripts that you planned to write when you proposed the research, or whether these should be published as separate publications, clearly identified as post hoc questions? The realities of publishing in scientific journals means that in many instances you will be restricted by the number of words a journal will accept. This will mean that for certain outputs it will not be possible for you to ask additional post hoc questions, or potentially all the questions you wanted to report in the [preregistered plan](#commitment). 

Note that salami-slicing is a completely different dual publication: publishing the same paper more than once in different journals. This should be considered self-plagiarism at best and fraud at worst. If you find examples of dual publications, these should result in [retractions](retract.html). Instances don't need to be exact copies. I edited a submission where one of the referees alerted me to the fact that the same data with a very similar question had already been published two years previously. In this instance, I passed the submission to the ethical panel of the journal who rejected it and flagged the authors for scrutiny in the case of future submissions. Note that instances where conference abstracts are printed in a journal does not prevent you from publishing a full paper of this work. I would maintain, however, that you should rewrite the abstract to prevent [self-plagiarism](http://howtowriteaphd.com/plagiarise.html) [@measey2021how]. 

Let me state from the outset that there is nothing wrong with writing papers based on findings that you came across during the study: post hoc questions. But that any publication (or part of a publication) that results must indicate that it results from a post hoc study. To my way of thinking, it would be more useful if journals had separate sections for such studies, with other publications only stemming from from those that can show a [preregistered plan](#commitment). This would clearly improve [transparency](transparency2.html) in publishing, and avoid accusations of [p-hacking or HARKing](transparency.html).

At what point does the separation of research questions into different papers become 'salami slicing': i.e. the multiplication of papers into as many papers as possible, or 'least publishable units'? There is no simple answer to this question, and editors are likely to disagree [@tolsgaard2019salami-slicing]. However, there are ways in which you as an author can make sure that your work is [transparent](transparency2.html), and therefore that you are not accused of 'salami slicing'. First is the [preregistration](#commitment) of your research plan. Second is to [preprint](preprints2.html) any unpublished papers that are referenced in your submission. There are also guidelines from COPE on the: '[Systematic manipulation of the publication process](https://publicationethics.org/resources/flowcharts/systematic-manipulation-publication-process)' [@cope2018systematic].

In manuscripts where another very similar study is cited by the authors, but not available to reviewers or editors, there should be a 'salami slicing' red flag. Obviously, when you produce a number of outputs from a research project, they are likely to be linked and therefore [cited by each other](#self). However, when these are not available to reviewers and editors (as [preprints](preprints2.html) or as [preregistration](#commitment) of the questions), authors should expect to be asked for these manuscripts to demonstrate that they are not salami-sclicing. Perhaps worse, however, is when authors deliberately hide any citation to another very similar work. In the end, we have to rely on the integrity of the researchers not to be unethical or dishonest. 


## Is writing a lot of papers a good strategy? 
This is a question of long standing, and one that you may find yourself asking at some point early on in your career. I'd suggest that the answer will be more about the sort of person that you are, or the lab culture you experience, over any strategy that you might consciously decide. If you tend toward perfectionism, this will likely result in fewer papers that (I hope) you'd consider to be of high quality. If on the other hand your desire were to finish projects and move on, you'd be more likely to tend toward more papers. It is clear that the current climate leads towards the latter strategy, with increasing numbers of early career researchers bewildered at the idea of increasing their publication metrics [@helmer2020what]. But what should you do?

Given that the 'best' personality type lies somewhere in the middle, you can decide for yourself whether you identify with one side more than the other. But which is the better strategy? Vincent Larivière and Rodrigo Costas [-@lariviere2016how] tried to answer this question by considering how many papers unique authors wrote and seeing how this relates to their share of authoring a paper in the top 1% of cited papers. Their result showed clearly that for researchers in the life sciences, writing a lot of papers was a good strategy if you started back in the 1980s. However, for those starting after 2009, the trend was reversed with those authors writing more papers less likely to have a 'smash hit' paper (in the top 1% of cited papers). Maybe the time scale was too short to know. After all, if you started publishing in 2009 and had >20 papers by 2013 then you have been incredibly prolific. Other studies continue to show that in the life sciences, writing more papers still provides returns towards having papers highly cited: the more papers you author, the higher the chance of having a highly cited paper [@sandstrom2016quantity]. 

One aspect not considered Larivière and Costas [-@lariviere2016how] is that becoming known as a researcher who finishes work (resulting in a publication) is likely to make you more attractive to collaborators. Thus, publishing work is likely to get you invited to participate in more work. Obviously, quality plays a part in invitations to collaborative work too. Thus pulling the argument back to the centre ground. 

There are other scenarios in which you might be encouraged to write more. In Denmark, for example, research funding is apportioned to universities based on the number of outputs their researchers generated in a point system, where higher ranked journals get more points. This resulted in researchers in the life sciences changing their publication strategy with a notable increase in publications in the highest points bracket following this change [@deutz2021quantitative]. 

If you find yourself becoming preoccupied about which is the best strategy for you, I'd suggest that you get back to finishing what you were writing before you got distracted!

### Natural selection of bad science {#badscience}
In 2016, Smaldino and McElreath proposed that ever increasing numbers of publications not only leads to bad science, but is currently selected for in an academic environment where publishing is considered as a currency. They argued that the most productive laboratories will be rewarded with more grant funding, larger numbers of students, and that these students will learn about the methods and benefits of prolific publication. When these ‘offspring’ of the prolific lab look for jobs, they are more likely to be successful as they have more publications themselves. An academic environment that rewards increasing numbers of publications eventually selects towards methodologies that produce the greatest number of publishable results. To suggest that this leads to a culture of ‘bad science’ Smaldino and McElreath [-@smaldino2016natural] conducted an analysis in trends over time of statistical power in behavioural science publications. Over time, better science should be shown by researchers increasing their statistical power as this will provide studies with lower error rates. However, increasing the statistical power of experiments takes more time and resources, resulting in fewer publications. Their results, from review papers in social and behavioural sciences, suggested that between 1960 and 2011 there had been no trend toward increasing statistical power. Biological systems, whether they be academics in a department or grass growing in experimental pots, will respond to the rewards generated in that system. When grant funding bodies and academic institutions reward publishing as a behaviour, it is inevitable that the behaviour of researchers inside that system will respond by increasing their publication output. Moreover, if those institutions maintain increasing numbers of researchers in temporary positions, those individuals are further incentivised to become more productive to justify their continued contracts, or the possibility of obtaining a (more permanent) position elsewhere. Eventually, this negative feedback, or gameification of publishing metrics, produces a dystopian and dysfunctional academic reality [@helmer2020what].

An example of this kind of confirmation bias driven publishing effect toward bad science can be found in the literature of fluctuating asymmetry, and in particular those studies on human faces [@dongen2011associations]. Back in the 1990s, there was a flurry of high profile articles purporting preference for symmetry (and against asymmetry) in human faces. The studies were (relatively) cheap and fast to conduct as the researchers had access to hundreds of students right on their doorsteps. The studies not only hit the top journals, but were very popular in the mainstream media as scientists were apparently able to predict which faces were the most attractive. Stefan van Dongen [-@dongen2011associations] hypothesised that if publication bias was leading to bad science in studies of fluctuating asymmetry in human faces, there would be a negative association between effect size and sample size when fluctuating asymmetry in human faces was not the main aim of the study. Ideally, he'd have compared published with unpublished studies, but the problem with unpublished studies is that they are very difficult to find. He found that when fluctuating asymmetry in human faces was not the main aim of the study, the effect sizes diminished, suggesting that there was important publication bias. Where others have looked, publication bias has been found and is particularly associated with a decreasing effect size that correlates with journal [Impact Factor](impactfactor.html): i.e. once the large eggect is published in a big journal, the natural selection of bad science results in publication bias, and diminishing effect sizes that ripple through lower impact factor journals [@munafo2007association; @brembs2013deep; @smaldino2016natural], while negative results disappear almost entirely [@fanelli2012negative].

The direct result of a system driven by Impact Factor and author publication metrics is that we will have a generation of scientists at the top institutions that are trained not to conduct the best science, but to generate publications that can be sold to the best journals. We should be deeply suspicious of any claim of linkage between top journals and quality [@brembs2013deep]. Indeed, what we see increasingly is that the potential rewards of publishing in top Impact Factor journals leads not only to bad science, but increasingly to [deliberate fraud](#pruittdata). Continuing along this path threatens to undermine the entire [scientific project](http://howtowriteaphd.com/lifescientific.html), and places science and scientists as just another stakeholder in a system ruled by economic markets, and their promotion of the fashion of the day [@brembs2013deep; @casadevall2012reforming].

## At what rate is the literature increasing?
A study using several databases (Web of Science, Scopus, Dimensions and Microsoft Academic) back to the beginning of their collections at the start of scientific journals in the mid 1600s. They suggest that the inflation rate of scientific literature runs at 4.02%, such that the literature will double in 16.8 years [@bornmann2020growth]. This means that there is literally twice as much published in 2020 as there was in 2003. 

Although the early period of scientific publishing was notably slower than today, it is since the mid-1940s (following the end of “World War II”) that science has seen an exponential growth in productivity, with annual growth of 5.1%, and a doubling time of 13.8 years [@bornmann2020growth].

## If more is being published, will Impact Factors increase?
Yes. If the numbers of citations per paper remains constant, then the [Impact Factor](impactfactor.html) of journals should increase annually at 5%. My impression is that citations are increasing in papers as the literature increases, which suggests that Impact Factor will grow at a faster rate. 
