# Transparency in publishing {#transparency2}

There is a great need for transparency in science, and one of the ways to achieve this is through [preregistration](#commitment) of your research project to avoid confirmation bias. In order to make this effective, we need our journals to support the preregistration of research hypotheses and methods. Right now, journals should be openly advocating and encouraging preregistration with a plan to transition their journal in future. However, many editors are resisting this move as they feel that there is no support from the community. This may well be the case, but as we have seen, inequalities in science, and particularly in publishing (see [Part IV](editors.html)), mean that editors can either be instruments of change, or at the heart of inequality in publishing. Either our editors will lead us toward transparency, or we as a community simply need to demand that they change their practices. Where it is happening, editors are responding to calls in transparency by making small steps [for example asking for open coding: @powers2019open], rather than adopting transparency wholesale through the badge system set up by Kidwell et al [-@kidwell2016badges].

## Removing the prejudice around confirmatory bias
Confirmatory bias is the phenomenon increasingly seen in science that most studies published accept the alternative hypothesis, even though this is the least likely outcome of any experiment. Confirmation bias happens in publishing as editors prefer to accept papers that have a positive outcome. It has been suggested that this leads to a culture of '[bad science](#badscience)', and even [fraud](#pruittdata). One convincing set of evidence of confirmation bias is the decline of null results over time [@fanelli2012negative].


### Accepting the alternative hypothesis
At the outset of our scientific research we pose a hypothesis with the expectation that we will be able to accept or reject our null hypothesis. We often think of rejecting the null hypothesis as the only result that we are interested in, but if we only ever reported these results we would not be responsible in moving our field forwards. That is, in a world where we only report significant results (i.e. reject the null hypothesis) we would necessarily keep repeating experiments where the null hypothesis is accepted, because there would never be the evidence that the hypothesis had been previously tested in the literature. This is called confirmation bias, and it’s actually practised by the majority of scientific journals who won’t consider a null result. It’s easy to see why this is a bad policy, but it is the prevailing culture in science. 

If journals only publish manuscripts that reject the null hypothesis [cf @franco2014publication], researchers are more likely to mine their data for positive results (P hacking), or re-write their hypothesis in order to reject the null (HARKing) [@measey2021how]. Deceptive practices such as p hacking, HARKing and [salami-slicing](#salami) are not in the interests of any journals [@forstmeier2017detecting]. 


### Inadvertent bias
But positive results don’t only come from deliberate manipulation of results. As humans we are predisposed towards positive results [@trivers2011folly], and these can come about through deliberate manipulation of results, there are plenty of reasons why researchers might reach a false positive outcome. Forstmeier et al [-@forstmeier2017detecting] draw attention to cryptic multiple tests during stepwise model simplification, and the two types of researcher degrees of freedom [_sensu_ @simmons2011false]: stopping rules and flexibility in analysis.

__Cryptic multiple tests during stepwise model simplification__ relates to the way in which adding predictors to models inflates the total number of models to test, making it necessary to adjust alpha accordingly (for repeated tests). However, Forstmeier and Schielzeth [-@forstmeier2017detecting] report that even with Bonferroni adjusted alpha levels, using random data they found that models with one significant effect happen around 70% of the time. The only way to keep this under control is to use sufficient sample sizes to maintain the power to distinguish between true positives and false positives. A handy rule of thumb from Field [-@field2013discovering] is that sample size needs to be 8 times the number of model predictors plus 50. Better would be to run a power analysis on your study design, and to critically reassess your predictors to eliminate as many as you can before you begin your study.

__Researcher degrees of freedom__ is the way that Simmons et al [-@simmons2011false] described ways in which researchers may inadvertently increase their chances of getting false positive results during analysis. The first is simply the way in which researchers decide to stop collecting data. Clearly, if preliminary collections showed a trend, but not a significant result, then collecting more data sounds like a good idea. However, as the collection of data is not independent (the first set is kept) then the first test is not independent of the second, and so the chance of getting a Type I error is cumulative. Even if multiple datasets are collected, those that are insignificant should also be considered and reported in order to get an unbiased estimate. The second major way in which analyses can turn out with false positives is through potentially infinite flexibility in analyses. There are lots of ways to analyse your data and given enough trials, it is quite likely that you’ll find one that gives you significant results. Moreover, on the road to conducting the test, there are many options that can change the outcome of the analysis:

- Inclusion or exclusion of an outlier
- Inclusion or exclusion of a covariate
- Transforming dependent variables. 
- Inclusion or exclusion of baseline measures
- Controlling for sex (or another variable) as a fixed effect
- Excluding individuals with incomplete datasets

The potential list of ways in which the outcome of your analysis could change quickly grows as the number of ways in which you could analyse the data also grows. But don’t despair. Transparent help is at hand.

### Novel research
One criterion for many journals is that the research should be novel. This is increasingly practiced by journal editors as you move up the Impact Factor levels (see below). Novelty sells (just think of the meaning of “new” in newspaper), and that’s the basis for selling higher stories from higher Impact Factor journals. We have already seen in part 2 the perils of testing unlikely hypotheses and how this inflates Type II errors as well as increasing the proportion of Type I errors. Novelty also stifles repeatability. If we can never repeat studies in science, then a fundamental tenet of the methodology is repressed. Reproducibility in science has received a lot of attention recently, as attempts to reproduce the results of highly cited research have failed (cites). This has been followed by general outrage among scientists that things should change [@anderson2007normative; @munafo2017manifesto], including a majority of those in biological sciences [@baker20161500]. The irony that these reports and requests are published in exactly the journals that will refuse to publish research that seeks to repeat work (is not novel) is clearly lost on the editors. However, more nuanced views are also coming forwards to actively introduce variable conditions and sampling of biological variation into the study design to more fully represent the nature of biological variation making studies more likely to be replicated [@voelkl2020reproducibility]. 

## Introducing transparency in peer review
As you will have already seen (above), the way in which editors choose and interpret reviewers can either reinforce their own prejudices, or help to make publication more open and transparent for everyone. The first step along this road is to move from double-blind review to triple-blind where editors cannot make decisions with prejudice towards certain reviewers (see above). Next is the need for open reviews with DOIs that allow open assessment of what reviews contained. For more details about problems in peer review, see above.

In order to change this culture to a more transparent selection of scientific studies for publishing, we need journals to sign up to be transparent. Sadly, when most journals are approached, the editors either ignore the email or make an excuse about why it is not possible (see here). Of course, some journals have adopted the road to transparency, and we should be encouraged by the fact that they still exist, and that we could build on these initial front runners.

## Removing profit from academic publishing
Taking out the profiteering from publishers will take a more concerted approach. But the reality is that we have only ourselves to blame. The publishers are able to prey on us, as biological scientists, because we are used to getting all of the "frills" associated with their publishing model. This includes the designer layout, custom websites and editorial management systems. 

But the reality is that we really don't need any of these frills, and if we cared more about our science and less about the prestige that publishers have worked so hard to con us into thinking that their products have higher value, we'd all be better off. Mathematicians and physisists are way ahead of us. Given that they've shown the way, it's simply up to us to [follow their models](#overlay).
