# The problems with peer review {#problempeerreview}

Some people have argued that peer review is untested and that the effects are uncertain [@jefferson2002effects]. Perhaps more worryingly, studies designed to test peer review (by deliberately sending out manuscripts with errors) have shown that most reviewers are unable to find all errors and some find none [@rothwell2000reproducibility]. 

Essentially, the major problem with peer review is that it is conducted by humans, and that like humans in societies everywhere, reviewers tend to have their own set of biases. The above sections should have given you some idea about the frailties of the peer review system. 

Demonstrated biases in peer review include the following:

- Evidence of bias against female authors is well established [e.g. @tregenza2002gender; @manlove2018authors], and against female reviewers [@helmer2017gender]
- Evidence of bias towards author reputation, favoring acceptance of manuscripts with poor reviews [@bravo2018hidden]
- Evidence of bias towards authors from more prestigious institutions [@tomkins2017single; @manlove2018authors], so-called prestige bias [@lee2013bias]
- Evidence for both nationality and language bias also exist in peer review [@lee2013bias; @manlove2018authors; @nunez2021monolingual]
- Confirmation bias (the tendency for journals and reviewers to favour significant results) is one of the biggest issues for our current system [see @fanelli2010positive and [part IV](transparency.html)]

But that’s not all, as this is only a set of biases that have already been investigated. Given that over 280 biases have already been catalogued (I encourage you to look through the [online catalogue](www.catalogofbias.org), many more different types of bias are likely to exist in peer review. Let’s not forget that our biases have evolved because they are very useful. They exist as a way of shortcutting exhaustive decision making based on random variables. But maybe peer review needs some more of this. And perhaps that means that I should be tolerant when I’m asked to review an economics journal, as these folk clearly weren’t exhibiting any biases associated with economists when they picked me (see [below](editors.html)).

Perhaps the biggest problem facing those who wish to reform the peer review system is that it all starts with editors who are choosing reviewers. Those editors themselves have their own inherent biases. When they look for reviewers, they are likely to sample from within their own group of peers who have the same biases. Interestingly, bias (in general) is more easily perceived by early career scientists [@zvereva2021biases]. My experience is that soliciting reviews from people that I don’t know and have no connection with (are outside of my field) are more likely to fail - they will say no, or they won’t reply to the request [@perry2012peer]. This is even for academics that are publishing within the same area. 

Editors are the people who select reviewers, and inspection of most editorial boards will reveal that they reflect the same biases found in peer review. That is editorial boards are mostly made up of white men from Europe and North America. Rectifying this bias will take time and the acknowledgement that there is a problem together with the willingness to do something about it. In 2020, I have seen that there has been a big movement to redress the imbalance in science at all levels. I hope that this will continue into the future so that at least some of the biases in peer review will fall away.

## All reviews are not equal
If you are an editor and you receive three reviews from three researchers each suggesting something different, I have argued (below) that the editor should make their own decision on what action to take. But what if one of the reviewers is very negative and is a leader in their field? Should their review count equally with the others? Should their opinion be given more weight than the others? Of course, they could be using their position to influence their field, to make sure that opinions they hold are reinforced. Lee et al [-@lee2013bias] provide a good overview of the potential way in which influential reviewers could bias the peer review system. But the power sits with the editor to make this decision. Interestingly, Thurner and Hanel [-@thurner2011peer] make the point using an agent based model (much as you might use in biological sciences) to show that only a small number of biased (for whatever reason) reviewers are needed to seriously degrade the quality of peer review, and thus the science system as a whole.

The truth is that all reviews are not equal because some reviewers will put in more effort than others. Some will know the literature better. Some will be experts in the field that should be better placed to comment. These people are actually more likely to be less senior, PhD students or post docs. However, the importance for the editor is not to take account of the names of these people, their rank, their institution, or other demographics such as their gender, race or nationality. There are great editors out there who can do this, but my impression is that the majority fail. In this case, the only way to do this is for the triple blind method. Here the editors will invite the reviewers (by name) but the reviews that result will not be marked with the reviewers’ names. This will make forgetting who they are easier for 

## Decisions rest with editors
A good editor will look at the reasoning in the reviews and make a decision in an unbiased way. A poor editor may be swayed by the perceived influence of an important reviewer irrespective of their argument. An increasing trend that I’ve noticed is that editors will simply take a decision that follows the consensus of all reviewers: that is, they rate all reviewers equally (see also Rothwell and Martyn 2000). However, I would argue that this is also bad editing. Irrespective of the bias from reviewers, guarding the integrity of the process of peer review lies with editors. 

Today, editors are so busy with the other duties that their jobs as academics that their decisions are hurried and expecting them to take the time and space to overcome their personal biases might be a lot to ask. Instead, I think that it is time for the triple review concept to move into the mainstream so that editors can more easily not be led by potential biases of their reviewers. 

Another important problem with peer review comes when editors are not independent of authors. This can happen when an editor is known well by the authors. They could be in the same department or even in the same research group. Similarly, there could be a group of editors for different journals that have some quid pro quo arrangement, that might even be unstated, whereby their manuscripts do not undergo equal scrutiny to other manuscripts that are submitted. One could argue that whenever editors know the names of the authors, there’s a possibility for the system to be corrupted. 

Despite all of the problems with peer review that are acknowledged above, we stick with it as the majority system in science. It could be that peer review favours exactly the same people who uphold the system and prevent it from moving into something more equal, just and fair. These are the editors and reviewers who have, for the most part, managed to make their careers inside the system, and have therefore mastered it to some degree. 

To you, dear reader, I can only suggest that you be aware of all the potential pitfalls with peer review, and never stop striving for something better. 

## The social side of peer review
There is so much more to peer review than peer review. Being selected by an editor to review a manuscript represents an important standing amongst your peers. Literally it means that your opinion is valued. But there's much more to it. Doing a good job at peer review means that you improve other people's work. This help can be valued to the point where those colleagues get in touch and want to work with you. That this can happen has now been shown in a study, and has been termed the 'invisible hand' of peer review.

### The 'invisible hand' of peer review
In another study, Dondio et al [-@dondio2019invisible] found that reviewers were more likely to provide positive review comments to authors who were close [≤3 steps] to their collaborative networks [see @adams2012rise]. In this case, a close reviewer to the author was calculated by a social network where a distance of 1 meant that they had co-authored together [1 step], co-authors of the reviewer may have collaborated with these authors [2 steps], or co-authors of reviewers and authors had collaborated [3 steps]. Surprisingly, they obtained this result even though the journal practiced a strict double-blind review system (reviewers didn’t know who the authors were, and vice versa - see part 4). Referees that were not close [i.e. ≥4 steps] were more likely to provide more negative review comments. Those who helped authors more during peer review (i.e. asked for major revisions), were more likely to cite the manuscript, once published, and eventually more likely, than random, to publish with those same authors, even if manuscripts were eventually rejected. The authors concluded therefore that peer review may accelerate the potential for collaboration in science.

This appears to be based on the fact that peer review can/should be constructive. Authors and peer reviewers are in fact collaborating to improve the quality of a manuscript. The process is orchestrated by an editor who can and should join in to improve the manuscript. Dondio et al [-@dondio2019invisible] make the point that this interaction is inherently social, and the peer review therefore has a function that develops relationships within and between networks of researchers. 

This evidence that peer review is a collaborative system towards the betterment of science is, to me, a sign that all is alive and well in science, and that peer review is acting as it should. However with any social network comes the fragilities of human bias. This means that while peer review may function well for some, for others it may more often than not fail. The bigger problem is that it might depend on your sex, the colour of your skin, the name of your institution, or your country as to whether you are selected as a potential reviewer (i.e. to join the club), or having submitted your manuscript, find that peer review is going to work for you. In addition, if you are never asked to review then you will never benefit from this network. 

Casnici et al [-@casnici2017assessing] tracked the fate of rejected manuscripts and showed that if the reviewers had several rounds of peer review before rejection, these manuscripts benefitted later by being accepted to journals with a higher Impact Factor, and/or obtaining greater numbers of citations, even if the reviewers were instrumental in rejecting the manuscript. This suggests that in working collaboratively on a manuscript, reviewers are more likely to promote, cite and help authors. The alternative is that reviewers agree to re-review an article again because they see merit in it, even if they also see flaws. And having spent considerable effort on manuscript, they are more likely to remember and cite it. But this doesn’t take away from the idea that reviewers and authors are collaborating in a social way.
